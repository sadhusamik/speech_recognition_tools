[cfg_proto]
cfg_proto = conf/proto/global.proto
cfg_proto_chunk = conf/proto/global_chunk.proto

[exp]
cmd = 
run_nn_script = ../../tools/pytorch-kaldi/run_nn.py 
out_folder = exp_noisy/hybrid_pykaldi_tri2_mfcc
seed = 1234
use_cuda = True
multi_gpu = False
save_gpumem = False
n_epochs_tr = 23

[dataset1]
data_name = train
fea = fea_name=mfcc
	fea_lst=./data/train/feats.scp
	fea_opts=apply-cmvn --utt2spk=ark:data/train/utt2spk scp:data/train/cmvn.scp ark:- ark:- |
	cw_left=4
	cw_right=4
	

lab = lab_name=lab_cd
	lab_folder=/export/b15/ssadhu/pyspeech/recipes/timit/exp_adapt/mono_ali_train
	lab_opts=ali-to-pdf
	lab_count_file=auto
	lab_data_folder=/export/b15/ssadhu/pyspeech/recipes/timit/data_adapt/train/
	lab_graph=/export/b15/ssadhu/pyspeech/recipes/timit/exp_adapt/mono/graph_test_bg
n_chunks = 1


[dataset2]
data_name = dev
fea = fea_name=mfcc
	fea_lst=/export/b15/ssadhu/pyspeech/recipes/timit/data_adapt/test/feats.scp
	fea_opts=apply-cmvn --utt2spk=ark:/export/b15/ssadhu/pyspeech/recipes/timit/data_adapt/test/utt2spk scp:/export/b15/ssadhu/pyspeech/recipes/timit/data_adapt/test/cmvn.scp ark:- ark:- | add-deltas ark:- ark:- |
	cw_left=4
	cw_right=4
	

lab = lab_name=lab_cd
	lab_folder=/export/b15/ssadhu/pyspeech/recipes/timit/exp_adapt/mono_ali_test
	lab_opts=ali-to-pdf
	lab_count_file=auto
	lab_data_folder=/export/b15/ssadhu/pyspeech/recipes/timit/data_adapt/test/
	lab_graph=/export/b15/ssadhu/pyspeech/recipes/timit/exp_adapt/mono/graph_test_bg
n_chunks=1	


[data_use]
train_with = train
valid_with = dev
forward_with = test

[batches]
batch_size_train = 128
max_seq_length_train = 1000
increase_seq_length_train = False
start_seq_len_train = 100
multply_factor_seq_len_train = 2
batch_size_valid = 128
max_seq_length_valid = 1000

[architecture1]
arch_name = MLP_layers1
arch_proto = conf/proto/MLP.proto
arch_library = neural_networks
arch_class = MLP
arch_pretrain_file = none
arch_freeze = False
arch_seq_model = False
dnn_lay = 256,256,256,256,256,N_out_lab_cd
dnn_drop = 0.15,0.15,0.15,0.15,0.0
dnn_use_laynorm_inp = False
dnn_use_batchnorm_inp = False
dnn_use_batchnorm = True,True,True,True,False
dnn_use_laynorm = False,False,False,False,False
dnn_act = tanh,tanh,tanh,tanh,tanh,softmax
arch_lr = 0.08
arch_halving_factor = 0.5
arch_improvement_threshold = 0.001
arch_opt = sgd
opt_momentum = 0.0
opt_weight_decay = 0.0
opt_dampening = 0.0
opt_nesterov = False


[model]
model_proto = conf/proto/model.proto
model = out_dnn1=compute(MLP_layers1,mfcc)
	loss_final=cost_nll(out_dnn1,lab_cd)
	err_final=cost_err(out_dnn1,lab_cd)

[forward]
forward_out = out_dnn1
normalize_posteriors = True
normalize_with_counts_from = lab_cd
save_out_file = False
require_decoding = True

[decoding]
decode_only = False
decoding_script_folder = ../../tools/pytorch-kaldi/kaldi_decoding_scripts/
decoding_script = decode_dnn_samik.sh
decoding_proto = conf/proto/decoding.proto
min_active = 200
max_active = 7000
max_mem = 50000000
beam = 13.0
latbeam = 8.0
acwt = 0.2
max_arcs = -1
skip_scoring = false
scoring_script = local/score.sh
scoring_opts = "--min-lmwt 1 --max-lmwt 10"
norm_vars = False

